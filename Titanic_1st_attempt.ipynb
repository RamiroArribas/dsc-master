{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to ML through the Titanic dataset on Kaggle\n",
    "\n",
    "The intention of this notebook is neither to achieve the best score nore to present the most efficient code, but to  walk the first steps in the Machine Learning path. Hence, instead of presenting a well structured notebook with colorful visualizations and detailed explanations I will just present what I've learned so far with some notes in the points I deem interesting.\n",
    "\n",
    "Because of the above, there's a lot of room for improvement so I might come in the future to give another go to the project. At the time of the submission, this notebook yielded an accuracy of 0.78947 as per the competition's leaderboard.\n",
    "\n",
    "I'd like to attach some links of kernels I found interesting because of the ideas they present (feature engineering, predictive models used, etc.):\n",
    "\n",
    "https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "Not a 99% accuracy kernel, but a comprehensive guide for data science beginners.\n",
    "\n",
    "https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n",
    "A kernel that introduces the concept of Ensembling/Stacking, a method responsible for many a team winning Kaggle competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries that we are going to use:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/../Titanic/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-77dac5d6e0eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Reading the train / test data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/../Titanic/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/../Titanic/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kschool/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kschool/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kschool/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kschool/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kschool/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/../Titanic/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#Reading the train / test data:\n",
    "\n",
    "train_data = pd.read_csv('/../Titanic/train.csv')\n",
    "test_data = pd.read_csv('/../Titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Murphy, Miss. Margaret Jane</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PassengerId    Survived      Pclass                         Name  \\\n",
       "count    891.000000  891.000000  891.000000                          891   \n",
       "unique          NaN         NaN         NaN                          891   \n",
       "top             NaN         NaN         NaN  Murphy, Miss. Margaret Jane   \n",
       "freq            NaN         NaN         NaN                            1   \n",
       "mean     446.000000    0.383838    2.308642                          NaN   \n",
       "std      257.353842    0.486592    0.836071                          NaN   \n",
       "min        1.000000    0.000000    1.000000                          NaN   \n",
       "25%      223.500000    0.000000    2.000000                          NaN   \n",
       "50%      446.000000    0.000000    3.000000                          NaN   \n",
       "75%      668.500000    1.000000    3.000000                          NaN   \n",
       "max      891.000000    1.000000    3.000000                          NaN   \n",
       "\n",
       "         Sex         Age       SibSp       Parch    Ticket        Fare  \\\n",
       "count    891  714.000000  891.000000  891.000000       891  891.000000   \n",
       "unique     2         NaN         NaN         NaN       681         NaN   \n",
       "top     male         NaN         NaN         NaN  CA. 2343         NaN   \n",
       "freq     577         NaN         NaN         NaN         7         NaN   \n",
       "mean     NaN   29.699118    0.523008    0.381594       NaN   32.204208   \n",
       "std      NaN   14.526497    1.102743    0.806057       NaN   49.693429   \n",
       "min      NaN    0.420000    0.000000    0.000000       NaN    0.000000   \n",
       "25%      NaN   20.125000    0.000000    0.000000       NaN    7.910400   \n",
       "50%      NaN   28.000000    0.000000    0.000000       NaN   14.454200   \n",
       "75%      NaN   38.000000    1.000000    0.000000       NaN   31.000000   \n",
       "max      NaN   80.000000    8.000000    6.000000       NaN  512.329200   \n",
       "\n",
       "          Cabin Embarked  \n",
       "count       204      889  \n",
       "unique      147        3  \n",
       "top     B96 B98        S  \n",
       "freq          4      644  \n",
       "mean        NaN      NaN  \n",
       "std         NaN      NaN  \n",
       "min         NaN      NaN  \n",
       "25%         NaN      NaN  \n",
       "50%         NaN      NaN  \n",
       "75%         NaN      NaN  \n",
       "max         NaN      NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's take a general look to our data:\n",
    "\n",
    "train_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>A6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>McKane, Mr. Peter David</td>\n",
       "      <td>male</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28403</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Waelens, Mr. Achille</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345767</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andreasson, Mr. Paul Edvin</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>347466</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>669</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Cook, Mr. Jacob</td>\n",
       "      <td>male</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 3536</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                          Name   Sex   Age  \\\n",
       "23            24         1       1  Sloper, Mr. William Thompson  male  28.0   \n",
       "397          398         0       2       McKane, Mr. Peter David  male  46.0   \n",
       "80            81         0       3          Waelens, Mr. Achille  male  22.0   \n",
       "91            92         0       3    Andreasson, Mr. Paul Edvin  male  20.0   \n",
       "668          669         0       3               Cook, Mr. Jacob  male  43.0   \n",
       "\n",
       "     SibSp  Parch    Ticket     Fare Cabin Embarked  \n",
       "23       0      0    113788  35.5000    A6        S  \n",
       "397      0      0     28403  26.0000   NaN        S  \n",
       "80       0      0    345767   9.0000   NaN        S  \n",
       "91       0      0    347466   7.8542   NaN        S  \n",
       "668      0      0  A/5 3536   8.0500   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005007</td>\n",
       "      <td>-0.035144</td>\n",
       "      <td>0.036847</td>\n",
       "      <td>-0.057527</td>\n",
       "      <td>-0.001652</td>\n",
       "      <td>0.012658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>-0.005007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.338481</td>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.257307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.035144</td>\n",
       "      <td>-0.338481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.369226</td>\n",
       "      <td>0.083081</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>-0.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.036847</td>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.369226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.308247</td>\n",
       "      <td>-0.189119</td>\n",
       "      <td>0.096067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.057527</td>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.083081</td>\n",
       "      <td>-0.308247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414838</td>\n",
       "      <td>0.159651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>-0.001652</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>-0.189119</td>\n",
       "      <td>0.414838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.257307</td>\n",
       "      <td>-0.549500</td>\n",
       "      <td>0.096067</td>\n",
       "      <td>0.159651</td>\n",
       "      <td>0.216225</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PassengerId  Survived    Pclass       Age     SibSp     Parch  \\\n",
       "PassengerId     1.000000 -0.005007 -0.035144  0.036847 -0.057527 -0.001652   \n",
       "Survived       -0.005007  1.000000 -0.338481 -0.077221 -0.035322  0.081629   \n",
       "Pclass         -0.035144 -0.338481  1.000000 -0.369226  0.083081  0.018443   \n",
       "Age             0.036847 -0.077221 -0.369226  1.000000 -0.308247 -0.189119   \n",
       "SibSp          -0.057527 -0.035322  0.083081 -0.308247  1.000000  0.414838   \n",
       "Parch          -0.001652  0.081629  0.018443 -0.189119  0.414838  1.000000   \n",
       "Fare            0.012658  0.257307 -0.549500  0.096067  0.159651  0.216225   \n",
       "\n",
       "                 Fare  \n",
       "PassengerId  0.012658  \n",
       "Survived     0.257307  \n",
       "Pclass      -0.549500  \n",
       "Age          0.096067  \n",
       "SibSp        0.159651  \n",
       "Parch        0.216225  \n",
       "Fare         1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we are browsing through our data, we can have a look at the correlation between features:\n",
    "\n",
    "train_data.corr()\n",
    "\n",
    "#Is worth to mention that there are a lot of informative plots to easily detect trends among features (for example, \n",
    "#heat maps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Survived\n",
      "0       1  0.629630\n",
      "1       2  0.472826\n",
      "2       3  0.242363\n"
     ]
    }
   ],
   "source": [
    "#We can also check the impact of diferent features in the survival probability:\n",
    "print (train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex  Survived\n",
      "0  female  0.742038\n",
      "1    male  0.188908\n"
     ]
    }
   ],
   "source": [
    "print (train_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "----------\n",
      "Test/Validation columns with null values:\n",
      " PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Let's find out which columns have missing values on both files:\n",
    "\n",
    "print('Train columns with null values:\\n', train_data.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with missing values, we have 3 options: drop columns, impute a value or impute a value with extension (adding a new column to represent in which lines the values were missing). It's important to consider how many rows of each feature have missing values. Embarked (the code of the embarking port) have 2 missing values, whereas Cabin have 687 and Age 177.\n",
    "\n",
    "The data type of the Cabin column is 'object', and since it has that many missing values, we'll just drop the column. Among other reasons, more than half the rows have missing values, and we don't have any method to infere the cabin code to impute them.\n",
    "\n",
    "In the Embarked column we just have 2 missing values. Since its data type is object and we just have 3 categories, we'll impute the mode.\n",
    "\n",
    "Lets dig a little bit more to decide what to do with 'Age'.\n",
    "\n",
    "#### Important note:\n",
    "Even though the 'imputation with extension' method (including extra columns showing what was imputed) is explained on the Kaggle ML tutorial, is worth mentioning that I haven't seen it applied in any of the various kernels reviewed. I have applied this method anyway for practicing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14., 10.,  6., 10.,  7.,  3.,  4.,  8.,  2.,  4.,  1.,  8.,  6.,\n",
       "        17., 13., 26., 25., 16., 51., 15., 31., 24., 18., 18., 27., 20.,\n",
       "        44., 18., 17., 16., 18., 23.,  6., 25., 13.,  8., 13.,  5.,  9.,\n",
       "        14., 12.,  9.,  6., 10.,  7.,  6.,  1.,  8.,  7.,  2.,  5.,  2.,\n",
       "         4.,  3.,  4.,  4.,  3.,  1.,  0.,  0.,  0.,  3.,  2.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  1.]),\n",
       " array([ 0.42      ,  1.55685714,  2.69371429,  3.83057143,  4.96742857,\n",
       "         6.10428571,  7.24114286,  8.378     ,  9.51485714, 10.65171429,\n",
       "        11.78857143, 12.92542857, 14.06228571, 15.19914286, 16.336     ,\n",
       "        17.47285714, 18.60971429, 19.74657143, 20.88342857, 22.02028571,\n",
       "        23.15714286, 24.294     , 25.43085714, 26.56771429, 27.70457143,\n",
       "        28.84142857, 29.97828571, 31.11514286, 32.252     , 33.38885714,\n",
       "        34.52571429, 35.66257143, 36.79942857, 37.93628571, 39.07314286,\n",
       "        40.21      , 41.34685714, 42.48371429, 43.62057143, 44.75742857,\n",
       "        45.89428571, 47.03114286, 48.168     , 49.30485714, 50.44171429,\n",
       "        51.57857143, 52.71542857, 53.85228571, 54.98914286, 56.126     ,\n",
       "        57.26285714, 58.39971429, 59.53657143, 60.67342857, 61.81028571,\n",
       "        62.94714286, 64.084     , 65.22085714, 66.35771429, 67.49457143,\n",
       "        68.63142857, 69.76828571, 70.90514286, 72.042     , 73.17885714,\n",
       "        74.31571429, 75.45257143, 76.58942857, 77.72628571, 78.86314286,\n",
       "        80.        ]),\n",
       " <a list of 70 Patch objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADu9JREFUeJzt3W+sZHV9x/H3pwuCoBbWvZAty/ZisrGQpiz2hmJpGgVrEQzwABqIMftgk31CU2hN7NImNSZ9AEkj+qBpsgHqprH8KaIQMCpZIaZ9gO4C6uJCQdzili27WijaJuritw/mbLxe7jJz5965M/Pb9yu5mTlnz+x8mDn3w29/c86ZVBWSpOn3a+MOIElaGRa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREnrOaTrVu3rmZnZ1fzKSVp6u3Zs+eHVTXTb7tVLfTZ2Vl27969mk8pSVMvyX8Msp1TLpLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1IiBzhRNsh/4MfA6cKSq5pKsBe4BZoH9wJ9U1SujialhzW5/+A3r9t9yxRiSSBq1pYzQ319Vm6tqrlveDuyqqk3Arm5ZkjQmy5lyuQrY2d3fCVy9/DiSpGENWugFfDXJniTbunVnVtVBgO72jFEElCQNZtCrLV5cVS8lOQN4JMkzgz5B9z+AbQAbN24cIqIkaRADjdCr6qXu9hDwBeBC4OUk6wG620PHeOyOqpqrqrmZmb6X85UkDalvoSc5Ncnbj94HPgjsBR4EtnSbbQEeGFVISVJ/g0y5nAl8IcnR7f+5qr6c5JvAvUm2Ai8C144upiSpn76FXlUvAOcvsv5HwKWjCCVJWjrPFJWkRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaMciXREsrYnb7w7+yvP+WK8aURGqTI3RJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNWLgQk+yJsmTSR7qls9J8niS55Lck+Qto4spSepnKSP0G4F985ZvBW6rqk3AK8DWlQwmSVqagQo9yQbgCuD2bjnAJcB93SY7gatHEVCSNJhBR+ifBj4O/KJbfifwalUd6ZYPAGct9sAk25LsTrL78OHDyworSTq2voWe5MPAoaraM3/1IpvWYo+vqh1VNVdVczMzM0PGlCT1M8g3Fl0MXJnkcuBk4B30RuynJTmhG6VvAF4aXUxJUj99R+hVdXNVbaiqWeA64GtV9RHgUeCabrMtwAMjSylJ6ms5x6H/JfAXSZ6nN6d+x8pEkiQNY0lfEl1VjwGPdfdfAC5c+UiSpGF4pqgkNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY04od8GSU4Gvg6c1G1/X1V9Isk5wN3AWuAJ4KNV9bNRhtVozG5/+FeW999yxZiSSFqOQUboPwUuqarzgc3AZUkuAm4FbquqTcArwNbRxZQk9dO30KvnJ93iid1PAZcA93XrdwJXjyShJGkgA82hJ1mT5CngEPAI8D3g1ao60m1yADhrNBElSYMYqNCr6vWq2gxsAC4Ezl1ss8Uem2Rbkt1Jdh8+fHj4pJKkN7Wko1yq6lXgMeAi4LQkRz9U3QC8dIzH7Kiquaqam5mZWU5WSdKb6FvoSWaSnNbdfyvwAWAf8ChwTbfZFuCBUYWUJPXX97BFYD2wM8kaev8DuLeqHkryXeDuJH8LPAncMcKckqQ++hZ6VX0buGCR9S/Qm0+XJE0AzxSVpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRg5xYJPW18Jrq4HXVpdXmCF2SGmGhS1IjLHRJaoRz6FPEeWpJb8YRuiQ1wkKXpEZY6JLUCOfQp9zCeXXn1Bfn66TjgSN0SWqEhS5JjbDQJakRzqHrDTzeXZpOjtAlqREWuiQ1wkKXpEY4h66p5zHmUo8jdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1Ij+hZ6krOTPJpkX5Knk9zYrV+b5JEkz3W3p48+riTpWAYZoR8BPlZV5wIXATckOQ/YDuyqqk3Arm5ZkjQmfQu9qg5W1RPd/R8D+4CzgKuAnd1mO4GrRxVSktTfkubQk8wCFwCPA2dW1UHolT5wxkqHkyQNbuCLcyV5G/B54Kaqei3JoI/bBmwD2Lhx4zAZNaUW+6IMSaMz0Ag9yYn0yvxzVXV/t/rlJOu7P18PHFrssVW1o6rmqmpuZmZmJTJLkhYxyFEuAe4A9lXVp+b90YPAlu7+FuCBlY8nSRrUIFMuFwMfBb6T5Klu3V8BtwD3JtkKvAhcO5qIkqRB9C30qvpX4FgT5peubBxJ0rA8U1SSGmGhS1IjLHRJaoRfEq2BjOKY8sX+zn5f8Oyx7dKxOUKXpEZY6JLUCAtdkhrhHPoEWTg/3G8+uUW+BtLwHKFLUiMsdElqhIUuSY1wDr0xrR2n3dp/jzRKjtAlqREWuiQ1wkKXpEY4h34ccl5aapMjdElqhIUuSY2w0CWpEVMzh+41PjQN3E81To7QJakRFrokNcJCl6RGTM0c+vHI48VHZ5C5bufDNW0coUtSIyx0SWqEhS5JjXAOXRoh5+G1mhyhS1IjLHRJaoSFLkmNsNAlqRF9Cz3JnUkOJdk7b93aJI8kea67PX20MSVJ/QwyQv8scNmCdduBXVW1CdjVLUuSxqhvoVfV14H/XrD6KmBnd38ncPUK55IkLdGwx6GfWVUHAarqYJIzjrVhkm3ANoCNGzcO+XRvtNh1ThYe4+sxwJpG7rca1sg/FK2qHVU1V1VzMzMzo346STpuDVvoLydZD9DdHlq5SJKkYQxb6A8CW7r7W4AHViaOJGlYfefQk9wFvA9Yl+QA8AngFuDeJFuBF4FrRxlSmkTDXK9+kM9+pGH1LfSquv4Yf3TpCmeRJC2DZ4pKUiMsdElqhNdD72Ol5jw9tniytfb9rc7VH58coUtSIyx0SWqEhS5JjTiu5tCdV9Q08nMcDcoRuiQ1wkKXpEZY6JLUiKbm0KfpWOJpyjptpu21XYlrwjgfLnCELknNsNAlqREWuiQ1oqk5dGmUpm1ufqk8T2P6OUKXpEZY6JLUCAtdkhphoUtSI/xQdAG/+Fc6tkFOaBrkd8jfj9FwhC5JjbDQJakRFrokNeK4n0Nv/WQRaZSG/f0ZxcXF/CzLEbokNcNCl6RGWOiS1Ijjfg59GM67qwWD7MfTvq8fb18E4ghdkhphoUtSIyx0SWrEsubQk1wGfAZYA9xeVbesSKoGTPvco6bLtO9v45rrHtWx6+P67xl6hJ5kDfD3wIeA84Drk5y3UsEkSUuznCmXC4Hnq+qFqvoZcDdw1crEkiQt1XIK/SzgB/OWD3TrJEljsJw59Cyyrt6wUbIN2NYt/iTJs0t8nnXAD5f4mNUyqdnMtTSTmgsmN9tIc+XWoR/6prkG+XuX8dxv9ncu9/X6zUE2Wk6hHwDOnre8AXhp4UZVtQPYMeyTJNldVXPDPn6UJjWbuZZmUnPB5GYz19KsVq7lTLl8E9iU5JwkbwGuAx5cmViSpKUaeoReVUeS/CnwFXqHLd5ZVU+vWDJJ0pIs6zj0qvoS8KUVynIsQ0/XrIJJzWaupZnUXDC52cy1NKuSK1Vv+BxTkjSFPPVfkhox0YWe5LIkzyZ5Psn2Mea4M8mhJHvnrVub5JEkz3W3p48h19lJHk2yL8nTSW6coGwnJ/lGkm912T7ZrT8nyeNdtnu6D9RXXZI1SZ5M8tCk5EqyP8l3kjyVZHe3bhLey9OS3JfkmW5fe++E5Hp391od/XktyU0Tku3Pu/1+b5K7ut+Hke9jE1voE3Zpgc8Cly1Ytx3YVVWbgF3d8mo7Anysqs4FLgJu6F6jScj2U+CSqjof2AxcluQi4Fbgti7bK8DWMWQDuBHYN295UnK9v6o2zzvEbRLey88AX66q3wLOp/e6jT1XVT3bvVabgd8F/g/4wrizJTkL+DNgrqp+m95BI9exGvtYVU3kD/Be4Cvzlm8Gbh5jnllg77zlZ4H13f31wLMT8Jo9APzRpGUDTgGeAH6P3skVJyz2Hq9ing30ftEvAR6id5LcJOTaD6xbsG6s7yXwDuD7dJ+3TUquRXJ+EPi3ScjGL8+iX0vvwJOHgD9ejX1sYkfoTP6lBc6sqoMA3e0Z4wyTZBa4AHicCcnWTWs8BRwCHgG+B7xaVUe6Tcb1nn4a+Djwi275nROSq4CvJtnTnWEN438v3wUcBv6xm6K6PcmpE5BroeuAu7r7Y81WVf8J/B3wInAQ+B9gD6uwj01yoQ90aQFBkrcBnwduqqrXxp3nqKp6vXr/HN5A72Ju5y622WpmSvJh4FBV7Zm/epFNx7GvXVxV76E3zXhDkj8cQ4aFTgDeA/xDVV0A/C/jmfY5pm4u+krgX8adBaCbs78KOAf4DeBUeu/pQiu+j01yoQ90aYExejnJeoDu9tA4QiQ5kV6Zf66q7p+kbEdV1avAY/Tm+U9LcvT8h3G8pxcDVybZT+8KoZfQG7GPOxdV9VJ3e4jeXPCFjP+9PAAcqKrHu+X76BX8uHPN9yHgiap6uVsed7YPAN+vqsNV9XPgfuD3WYV9bJILfdIvLfAgsKW7v4Xe/PWqShLgDmBfVX1qwrLNJDmtu/9Wejv5PuBR4JpxZauqm6tqQ1XN0tunvlZVHxl3riSnJnn70fv05oT3Mub3sqr+C/hBknd3qy4FvjvuXAtczy+nW2D82V4ELkpySvc7evQ1G/0+Ns4PMgb4cOFy4N/pzb3+9Rhz3EVvLuzn9EYsW+nNu+4Cnutu144h1x/Q+2fbt4Gnup/LJyTb7wBPdtn2An/TrX8X8A3geXr/RD5pjO/r+4CHJiFX9/zf6n6ePrq/T8h7uRnY3b2XXwROn4RcXbZTgB8Bvz5v3dizAZ8Enun2/X8CTlqNfcwzRSWpEZM85SJJWgILXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRvw/iMI7jAtAJYcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's have a look to the 'Age' data, dropping de nan values and ploting an histogram to get an idea of its distribution.\n",
    "\n",
    "plt.hist(train_data['Age'].dropna(), bins = 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing seems to indicate otherwise so we will assume the age feature follows a normal distribution.\n",
    "\n",
    "### Data cleaning: correcting, completing, creating and converting\n",
    "\n",
    "From the insights we got inspecting the data, for this 1st submission attempt we have taken the following decissions:\n",
    "\n",
    "- We will drop the Cabin, ticket and Name features. A note for future consideration: with feature engineering from the Name column we could extract the sex of the passenger, its title and as a derivate from it, its socio-economic status (SES).\n",
    "\n",
    "- We will impute the mode for the embarked missing values\n",
    "\n",
    "- We will impute the mean for the Fare missing value on the test dataset\n",
    "\n",
    "- We will impute the mean for the Age missing values. Instead of just imputing a statistic, there are some alternatives:\n",
    "    · Imputing the mean + random noise using standar deviation.\n",
    "    · A refined imputer method considering the high correlation showed with other features. Example: some kernels use Pclass as a point of reference. For the missing values of passengers belonging to a certain Pclass, we could apply the above method extracting the statistics from that subgroup.\n",
    "    \n",
    "Other possibilities for feature engineering could be to create a variable extracting the title from the Name and codifying it as a categorical value, creating a feature with the family size (inferred from the SibSp and Parch variables) or to create a variable showing if a passenger travels alone (expressed as a lack of realitves aboard). In some of the reviewed kernels, the features 'Age' and 'Fare' where transformed using the pd.qcut and pd.cut functions to divide them into ranges as a way to reduce the noise.\n",
    "\n",
    "On a side note, none of the kernels reviewed included the 'PassengerId' as a feature for the models but we'll not drop the column because the models where tested prior to investigating other solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_traind = train_data.copy()\n",
    "mod_testd = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_traind.drop(['Cabin', 'Ticket', 'Name'], inplace=True, axis=1)\n",
    "mod_testd.drop(['Cabin', 'Ticket', 'Name'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we will use the 'imputation with extension' method for both Age and Embarked\n",
    "#we will first create the 'was missing' columns.\n",
    "\n",
    "mod_traind['Age_was_missing'] = mod_traind['Age'].isnull()\n",
    "mod_testd['Age_was_missing'] = mod_testd['Age'].isnull()\n",
    "mod_traind['Embarked_was_missing'] = mod_traind['Embarked'].isnull()\n",
    "mod_testd['Embarked_was_missing'] = mod_testd['Embarked'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_traind['Age'] = SimpleImputer().fit_transform(mod_traind[['Age']])\n",
    "mod_testd['Age'] = SimpleImputer().fit_transform(mod_testd[['Age']])\n",
    "mod_traind['Embarked'] = SimpleImputer(strategy='most_frequent').fit_transform(mod_traind[['Embarked']])\n",
    "mod_testd['Embarked'] = SimpleImputer(strategy='most_frequent').fit_transform(mod_testd[['Embarked']])\n",
    "mod_testd['Fare'] = SimpleImputer().fit_transform(mod_testd[['Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId             False\n",
      "Survived                False\n",
      "Pclass                  False\n",
      "Sex                     False\n",
      "Age                     False\n",
      "SibSp                   False\n",
      "Parch                   False\n",
      "Fare                    False\n",
      "Embarked                False\n",
      "Age_was_missing         False\n",
      "Embarked_was_missing    False\n",
      "dtype: bool PassengerId             False\n",
      "Pclass                  False\n",
      "Sex                     False\n",
      "Age                     False\n",
      "SibSp                   False\n",
      "Parch                   False\n",
      "Fare                    False\n",
      "Embarked                False\n",
      "Age_was_missing         False\n",
      "Embarked_was_missing    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#A quick check to see if everything is OK:\n",
    "print(mod_traind.isnull().any(), mod_testd.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only step left prior to implementing the models would be to transform the categorical data into a suitable format for the ML learning models. To transform these features to numeric variables there are methods such as One-Hot Encoding.\n",
    "\n",
    "Even though the encoding algorithms are fairly easy to use, we will encode the 'Sex' data manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsc/anaconda3/envs/kschool/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/dsc/anaconda3/envs/kschool/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/dsc/anaconda3/envs/kschool/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/dsc/anaconda3/envs/kschool/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#In order to make the changes inplace we can use a for loop:\n",
    "for e in range(len(mod_traind['Sex'])):\n",
    "    if mod_traind['Sex'][e] == 'male':\n",
    "        mod_traind['Sex'][e] = 0\n",
    "    else: \n",
    "        mod_traind['Sex'][e] = 1\n",
    "        \n",
    "for e in range(len(mod_testd['Sex'])):\n",
    "    if mod_testd['Sex'][e] == 'male':\n",
    "        mod_testd['Sex'][e] = 0\n",
    "    else: \n",
    "        mod_testd['Sex'][e] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_was_missing</th>\n",
       "      <th>Embarked_was_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass Sex   Age  SibSp  Parch     Fare Embarked  \\\n",
       "0            1         0       3   0  22.0      1      0   7.2500        S   \n",
       "1            2         1       1   1  38.0      1      0  71.2833        C   \n",
       "2            3         1       3   1  26.0      0      0   7.9250        S   \n",
       "3            4         1       1   1  35.0      1      0  53.1000        S   \n",
       "4            5         0       3   0  35.0      0      0   8.0500        S   \n",
       "\n",
       "   Age_was_missing  Embarked_was_missing  \n",
       "0            False                 False  \n",
       "1            False                 False  \n",
       "2            False                 False  \n",
       "3            False                 False  \n",
       "4            False                 False  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_traind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId               int64\n",
       "Survived                  int64\n",
       "Pclass                    int64\n",
       "Sex                      object\n",
       "Age                     float64\n",
       "SibSp                     int64\n",
       "Parch                     int64\n",
       "Fare                    float64\n",
       "Embarked                 object\n",
       "Age_was_missing            bool\n",
       "Embarked_was_missing       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we can see, the values have changed, but have the data type changed as well?\n",
    "#Let's find out:\n",
    "mod_traind.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId               int64\n",
       "Survived                  int64\n",
       "Pclass                    int64\n",
       "Sex                       int64\n",
       "Age                     float64\n",
       "SibSp                     int64\n",
       "Parch                     int64\n",
       "Fare                    float64\n",
       "Embarked                 object\n",
       "Age_was_missing            bool\n",
       "Embarked_was_missing       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ALERT: even though we have changed the values in the 'Sex' feature, the dtype is still object. \n",
    "#If we apply the OHE method now, it will take Sex as a dummy (even having been encoded to 0 & 1),\n",
    "#so we have to transform its data type before the OHE process.\n",
    "mod_traind['Sex'] = pd.to_numeric(mod_traind['Sex'])\n",
    "mod_testd['Sex'] = pd.to_numeric(mod_testd['Sex'])\n",
    "mod_traind.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age_was_missing</th>\n",
       "      <th>Embarked_was_missing</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1073</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>964</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76.2917</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1077</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1091</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>30.27259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1125</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass  Sex       Age  SibSp  Parch     Fare  \\\n",
       "181         1073       1    0  37.00000      1      1  83.1583   \n",
       "72           964       3    1  29.00000      0      0   7.9250   \n",
       "48           940       1    1  60.00000      0      0  76.2917   \n",
       "185         1077       2    0  40.00000      0      0  16.0000   \n",
       "199         1091       3    1  30.27259      0      0   8.1125   \n",
       "\n",
       "     Age_was_missing  Embarked_was_missing  Embarked_C  Embarked_Q  Embarked_S  \n",
       "181            False                 False           1           0           0  \n",
       "72             False                 False           0           0           1  \n",
       "48             False                 False           1           0           0  \n",
       "185            False                 False           0           0           1  \n",
       "199             True                 False           0           0           1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_traind_ohe = pd.get_dummies(mod_traind)\n",
    "mod_testd_ohe = pd.get_dummies(mod_testd)\n",
    "mod_testd_ohe.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first approach to prepare our data is ready. Let's try some models\n",
    "\n",
    "Prior to the design of our models, since the Kaggle dataset doesn't supply the column Survived in the test data, we will split our train data to test the effectiveness of one model or another. To avoid bias, we will first randomize the entries and afterwards we will do the split.\n",
    "\n",
    "There are algorithms designed for the split train/test functionality, but again we will do it manually for the sake of practice. We will establish the random seed at 42 for reproducibility.\n",
    "\n",
    "#### Important note:\n",
    "Another strategy used to test the accuracy of the models used is the Cross Validation method. Is more time-consuming and intense in terms of computer resources but is most recommended for small dataset such as the one we are using for this project. Might be worth to try cross-validation in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index_list = [x for x in range(len(mod_traind_ohe))]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(random_index_list)\n",
    "data = mod_traind_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will reorganize the columns in order to split X and y (the same way the columns of the test data are organized):\n",
    "cols = mod_traind_ohe.columns.tolist()\n",
    "cols_sorted = [cols[1]] + [cols[0]] + cols[2:]\n",
    "data = mod_traind_ohe[cols_sorted].values\n",
    "data = data[random_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll assign 600 entries for train and the rest for test. Afterwards, we will split between X and y.\n",
    "x_train, x_test = np.array(data[:600]), np.array(data[600:])\n",
    "separate = lambda x: (x[:,1:], x[:,0])\n",
    "x_train, y_train = separate(x_train)\n",
    "x_test, y_test = separate(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our original train data separated into x_train, y_train, x_test and y_test.\n",
    "\n",
    "### Let the fun begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tree = DecisionTreeRegressor(random_state=1)\n",
    "titanic_tree.fit(x_train, y_train)\n",
    "val_tree_predictions = titanic_tree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 0., 0., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check our predictions. This is important, because we want a binary classifier (either 0 or 1) but the\n",
    "#outpu of some models will be values in between (or even above of 1 or below of 0).\n",
    "val_tree_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validations MAE:  73 / 291 = 0.2508591065292096\n"
     ]
    }
   ],
   "source": [
    "val_mae = mean_absolute_error(y_test, val_tree_predictions)\n",
    "counter_difpreds = 0\n",
    "for e in range(len(val_tree_predictions)):\n",
    "    if val_tree_predictions[e] != y_test[e]:\n",
    "        counter_difpreds += 1\n",
    "print('Validations MAE:  {} / 291 = {}'.format(counter_difpreds, val_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further changes, this model has already almost a 75% of accuracy. Let's try to find the maximum number of leaf nodes that gives the best accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_leaf_nodes: 5\t\t Mean Absolute error: 0.2851296836465368\n",
      "Max_leaf_nodes: 25\t\t Mean Absolute error: 0.26173845115122657\n",
      "Max_leaf_nodes: 50\t\t Mean Absolute error: 0.2474062612045138\n",
      "Max_leaf_nodes: 100\t\t Mean Absolute error: 0.25773343555017925\n",
      "Max_leaf_nodes: 250\t\t Mean Absolute error: 0.2508591065292096\n",
      "Max_leaf_nodes: 500\t\t Mean Absolute error: 0.2508591065292096\n"
     ]
    }
   ],
   "source": [
    "def get_mae(max_leaf_nodes, x_train, x_test, y_train, y_test):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(x_train, y_train)\n",
    "    preds_val = model.predict(x_test)\n",
    "    mae = mean_absolute_error(y_test, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n",
    "nodes_mae = []\n",
    "for _ in candidate_max_leaf_nodes:\n",
    "    leafs_mae = get_mae(_, x_train, x_test, y_train, y_test)\n",
    "    print(('Max_leaf_nodes: {}\\t\\t Mean Absolute error: {}').format(_,leafs_mae))\n",
    "    nodes_mae.append([_, leafs_mae])\n",
    "\n",
    "# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\n",
    "best_tree_size = min(nodes_mae, key = lambda x: x[1])[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10619469, 1.        , 0.10619469, 0.05833333, 0.        ,\n",
       "       0.10619469, 1.        , 0.88235294, 1.        , 0.10619469])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_best_tree = DecisionTreeRegressor(max_leaf_nodes = 50, random_state = 1)\n",
    "titanic_best_tree.fit(x_train, y_train)\n",
    "best_tree_predictions = titanic_best_tree.predict(x_test)\n",
    "best_tree_predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALERT!!!\n",
    "\n",
    "After adjusting the size of the tree, the first thing we realize is that the predicted values have changed to values between 0 and 1. From what we've read, there must be a way to specify the need of a binary output in our predictions.\n",
    "\n",
    "Since we've not deeply explored the models, for the time being we will format the results rounding them to 0 or 1 in order to compare our results with y_test. This is NOT an optimal method, and we're not sure if it's a correct one for that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validations MAE:  57 / 291 = 0.1958762886597938\n"
     ]
    }
   ],
   "source": [
    "btp_mod = []\n",
    "for e in best_tree_predictions:\n",
    "    if e < 0.5:\n",
    "        btp_mod.append(0)\n",
    "    else:\n",
    "        btp_mod.append(1)\n",
    "        \n",
    "val_mae_besttree = mean_absolute_error(y_test, btp_mod)\n",
    "counter_difpreds = 0\n",
    "for e in range(len(btp_mod)):\n",
    "    if btp_mod[e] != y_test[e]:\n",
    "        counter_difpreds += 1\n",
    "print('Validations MAE:  {} / 291 = {}'.format(counter_difpreds, val_mae_besttree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE for Random Forest Model: 0.2649484536082474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsc/anaconda3/envs/kschool/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "rf_model.fit(x_train, y_train)\n",
    "rf_predictions = rf_model.predict(x_test)\n",
    "rf_val_mae = mean_absolute_error(y_test,rf_model.predict(x_test))\n",
    "\n",
    "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 1. , 0.3, 0. , 0.6, 0. , 0.1, 1. , 1. , 0. ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the predictions with the regression tree, we observe that the predictions with the random forest give a float of just 1 decimal. Let's try and adjust these values the same way we did with the results of the tree decision plus (with max_leaf_nodes adjusted) to make the comparison with y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validations MAE:  62 / 291 = 0.21305841924398625\n"
     ]
    }
   ],
   "source": [
    "rf_mod_predictions = []\n",
    "for e in rf_predictions:\n",
    "    if e < 0.5:\n",
    "        rf_mod_predictions.append(0)\n",
    "    else:\n",
    "        rf_mod_predictions.append(1)\n",
    "\n",
    "val_mae_bestforest = mean_absolute_error(y_test, rf_mod_predictions)\n",
    "counter_difpreds = 0\n",
    "for e in range(len(rf_mod_predictions)):\n",
    "    if rf_mod_predictions[e] != y_test[e]:\n",
    "        counter_difpreds += 1\n",
    "print('Validations MAE:  {} / 291 = {}'.format(counter_difpreds, val_mae_bestforest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "from xgboost import XGBRegressor\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(x_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14953312, 1.1493738 , 0.14689901, 0.08344692, 0.45340788,\n",
       "       0.0792416 , 0.36567178, 0.7491114 , 0.8322771 , 0.15896991],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictions = xgb_model.predict(x_test)\n",
    "xgb_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validations MAE:  53 / 291 = 0.18213058419243985\n"
     ]
    }
   ],
   "source": [
    "xgb_mod_pred = []\n",
    "for i in range(len(x_test)):\n",
    "    if xgb_predictions[i] < 0.5:\n",
    "        xgb_mod_pred.append(0)\n",
    "    else:\n",
    "        xgb_mod_pred.append(1)\n",
    "        \n",
    "val_mae_xgbmodel = mean_absolute_error(y_test, xgb_mod_pred)\n",
    "counter_difpreds = 0\n",
    "for e in range(len(xgb_mod_pred)):\n",
    "    if xgb_mod_pred[e] != y_test[e]:\n",
    "        counter_difpreds += 1\n",
    "print('Validations MAE:  {} / 291 = {}'.format(counter_difpreds, val_mae_xgbmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost with model tuning\n",
    "\n",
    "There are some parameters, as the n_estimators, which we can adjust to reach a better performance. As learned from the ML Kaggle course, we'll find the n_estimators that better perform with 10 early_stopping_rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.477032\n",
      "Will train until validation_0-rmse hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-rmse:0.458828\n",
      "[2]\tvalidation_0-rmse:0.441986\n",
      "[3]\tvalidation_0-rmse:0.429146\n",
      "[4]\tvalidation_0-rmse:0.418603\n",
      "[5]\tvalidation_0-rmse:0.409471\n",
      "[6]\tvalidation_0-rmse:0.402055\n",
      "[7]\tvalidation_0-rmse:0.396874\n",
      "[8]\tvalidation_0-rmse:0.390633\n",
      "[9]\tvalidation_0-rmse:0.384916\n",
      "[10]\tvalidation_0-rmse:0.38223\n",
      "[11]\tvalidation_0-rmse:0.378658\n",
      "[12]\tvalidation_0-rmse:0.375418\n",
      "[13]\tvalidation_0-rmse:0.373405\n",
      "[14]\tvalidation_0-rmse:0.371255\n",
      "[15]\tvalidation_0-rmse:0.369365\n",
      "[16]\tvalidation_0-rmse:0.367792\n",
      "[17]\tvalidation_0-rmse:0.3664\n",
      "[18]\tvalidation_0-rmse:0.365769\n",
      "[19]\tvalidation_0-rmse:0.365443\n",
      "[20]\tvalidation_0-rmse:0.365345\n",
      "[21]\tvalidation_0-rmse:0.365086\n",
      "[22]\tvalidation_0-rmse:0.363967\n",
      "[23]\tvalidation_0-rmse:0.36404\n",
      "[24]\tvalidation_0-rmse:0.363726\n",
      "[25]\tvalidation_0-rmse:0.363648\n",
      "[26]\tvalidation_0-rmse:0.363812\n",
      "[27]\tvalidation_0-rmse:0.363808\n",
      "[28]\tvalidation_0-rmse:0.363981\n",
      "[29]\tvalidation_0-rmse:0.364315\n",
      "[30]\tvalidation_0-rmse:0.364109\n",
      "[31]\tvalidation_0-rmse:0.363332\n",
      "[32]\tvalidation_0-rmse:0.363373\n",
      "[33]\tvalidation_0-rmse:0.36373\n",
      "[34]\tvalidation_0-rmse:0.363902\n",
      "[35]\tvalidation_0-rmse:0.363743\n",
      "[36]\tvalidation_0-rmse:0.363318\n",
      "[37]\tvalidation_0-rmse:0.363922\n",
      "[38]\tvalidation_0-rmse:0.364161\n",
      "[39]\tvalidation_0-rmse:0.364267\n",
      "[40]\tvalidation_0-rmse:0.364647\n",
      "[41]\tvalidation_0-rmse:0.364739\n",
      "[42]\tvalidation_0-rmse:0.364931\n",
      "[43]\tvalidation_0-rmse:0.365245\n",
      "[44]\tvalidation_0-rmse:0.365346\n",
      "[45]\tvalidation_0-rmse:0.36572\n",
      "[46]\tvalidation_0-rmse:0.365692\n",
      "Stopping. Best iteration:\n",
      "[36]\tvalidation_0-rmse:0.363318\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbplus_model = XGBRegressor(n_estimators=1000)\n",
    "xgbplus_model.fit(x_train, y_train, early_stopping_rounds=10,\n",
    "                 eval_set=[(x_test, y_test)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbplus_tunned_model = XGBRegressor(n_estimators=36)\n",
    "xgbplus_tunned_model.fit(x_train, y_train, verbose=False)\n",
    "xgbplus_tunned_predictions = xgbplus_tunned_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validations MAE:  54 / 291 = 0.18556701030927836\n"
     ]
    }
   ],
   "source": [
    "xgbplus_mod_tunned_pred = []\n",
    "for i in range(len(x_test)):\n",
    "    if xgbplus_tunned_predictions[i] < 0.5:\n",
    "        xgbplus_mod_tunned_pred.append(0)\n",
    "    else:\n",
    "        xgbplus_mod_tunned_pred.append(1)\n",
    "\n",
    "val_mae_xgbplusmodel = mean_absolute_error(y_test, xgbplus_mod_tunned_pred)\n",
    "counter_difpreds = 0\n",
    "for e in range(len(xgbplus_mod_tunned_pred)):\n",
    "    if xgbplus_mod_tunned_pred[e] != y_test[e]:\n",
    "        counter_difpreds += 1\n",
    "print('Validations MAE:  {} / 291 = {}'.format(counter_difpreds, val_mae_xgbplusmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is worse than the 'base' model as per our measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariable regression model\n",
    "\n",
    "##### Disclaimer:\n",
    "Not sure this would be considered a ML model. We implemented the mutlivariable regression model in the Introduction to Statistics course as a way to introduce us to autograd. From that, it was recommended to explore tensorflow and keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autograd\n",
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232.97801869310334\n",
      "232.95604355189184\n",
      "232.93407457466108\n",
      "232.91211175970616\n",
      "232.89015510532323\n",
      "232.86820460980866\n",
      "232.84626027145885\n",
      "232.82432208857193\n",
      "232.80239005944463\n",
      "232.7804641823762\n",
      "232.7585444556649\n",
      "232.73663087760988\n",
      "232.71472344651087\n",
      "232.69282216066802\n",
      "232.67092701838186\n",
      "232.64903801795333\n",
      "232.6271551576844\n",
      "232.6052784358764\n",
      "232.58340785083215\n",
      "232.5615434008548\n",
      "232.5396850842475\n",
      "232.51783289931342\n",
      "232.4959868443582\n",
      "232.4741469176857\n",
      "232.45231311760114\n",
      "232.4304854424113\n",
      "232.40866389042063\n",
      "232.38684845993723\n",
      "232.36503914926783\n",
      "232.34323595671955\n",
      "232.32143888060074\n",
      "232.29964791921984\n",
      "232.277863070886\n",
      "232.25608433390875\n",
      "232.23431170659754\n",
      "232.21254518726278\n",
      "232.19078477421547\n",
      "232.1690304657673\n",
      "232.14728226022922\n",
      "232.12554015591394\n",
      "232.10380415113414\n",
      "232.08207424420291\n",
      "232.06035043343365\n",
      "232.03863271714044\n",
      "232.0169210936381\n",
      "231.99521556124154\n",
      "231.97351611826633\n",
      "231.9518227630277\n",
      "231.93013549384312\n",
      "231.90845430902823\n",
      "231.88677920690154\n",
      "231.86511018578017\n",
      "231.84344724398213\n",
      "231.82179037982667\n",
      "231.80013959163244\n",
      "231.77849487771937\n",
      "231.75685623640788\n",
      "231.7352236660177\n",
      "231.71359716487035\n",
      "231.69197673128704\n",
      "231.67036236359021\n",
      "231.6487540601019\n",
      "231.62715181914467\n",
      "231.6055556390418\n",
      "231.58396551811742\n",
      "231.56238145469607\n",
      "231.54080344710158\n",
      "231.5192314936598\n",
      "231.49766559269582\n",
      "231.47610574253622\n",
      "231.45455194150736\n",
      "231.43300418793615\n",
      "231.41146248014974\n",
      "231.38992681647667\n",
      "231.36839719524465\n",
      "231.34687361478282\n",
      "231.32535607342072\n",
      "231.30384456948758\n",
      "231.28233910131425\n",
      "231.26083966723047\n",
      "231.23934626556826\n",
      "231.21785889465815\n",
      "231.19637755283367\n",
      "231.17490223842566\n",
      "231.15343294976844\n",
      "231.13196968519506\n",
      "231.1105124430382\n",
      "231.0890612216342\n",
      "231.06761601931626\n",
      "231.04617683442032\n",
      "231.0247436652817\n",
      "231.00331651023706\n",
      "230.98189536762249\n",
      "230.9604802357754\n",
      "230.9390711130334\n",
      "230.91766799773356\n",
      "230.89627088821592\n",
      "230.87487978281797\n",
      "230.85349467987936\n",
      "230.83211557774044\n",
      "230.81074247474098\n",
      "230.7893753692219\n",
      "230.76801425952434\n",
      "230.74665914399012\n",
      "230.72531002096068\n",
      "230.70396688877926\n",
      "230.68262974578823\n",
      "230.66129859033174\n",
      "230.63997342075308\n",
      "230.61865423539678\n",
      "230.59734103260826\n",
      "230.57603381073181\n",
      "230.5547325681136\n",
      "230.5334373031002\n",
      "230.5121480140373\n",
      "230.49086469927295\n",
      "230.46958735715415\n",
      "230.44831598602863\n",
      "230.42705058424588\n",
      "230.40579115015402\n",
      "230.38453768210226\n",
      "230.36329017844082\n",
      "230.3420486375198\n",
      "230.32081305768972\n",
      "230.2995834373023\n",
      "230.27835977470878\n",
      "230.25714206826123\n",
      "230.2359303163118\n",
      "230.2147245172145\n",
      "230.19352466932196\n",
      "230.17233077098794\n",
      "230.15114282056732\n",
      "230.12996081641487\n",
      "230.1087847568848\n",
      "230.0876146403344\n",
      "230.0664504651186\n",
      "230.04529222959468\n",
      "230.02413993211928\n",
      "230.0029935710498\n",
      "229.98185314474543\n",
      "229.96071865156227\n",
      "229.9395900898609\n",
      "229.91846745800012\n",
      "229.89735075433993\n",
      "229.87623997724023\n",
      "229.85513512506193\n",
      "229.8340361961653\n",
      "229.81294318891358\n",
      "229.79185610166698\n",
      "229.77077493278892\n",
      "229.74969968064212\n",
      "229.72863034359005\n",
      "229.70756691999637\n",
      "229.6865094082248\n",
      "229.66545780664129\n",
      "229.6444121136101\n",
      "229.62337232749664\n",
      "229.60233844666791\n",
      "229.5813104694895\n",
      "229.56028839432815\n",
      "229.5392722195522\n",
      "229.51826194352952\n",
      "229.49725756462732\n",
      "229.47625908121543\n",
      "229.45526649166212\n",
      "229.4342797943379\n",
      "229.41329898761168\n",
      "229.39232406985553\n",
      "229.37135503943927\n",
      "229.3503918947347\n",
      "229.32943463411354\n",
      "229.30848325594846\n",
      "229.28753775861168\n",
      "229.26659814047682\n",
      "229.2456643999175\n",
      "229.22473653530793\n",
      "229.2038145450228\n",
      "229.18289842743624\n",
      "229.16198818092408\n",
      "229.14108380386284\n",
      "229.12018529462858\n",
      "229.09929265159786\n",
      "229.07840587314755\n",
      "229.057524957656\n",
      "229.036649903501\n",
      "229.0157807090612\n",
      "228.9949173727159\n",
      "228.97405989284422\n",
      "228.9532082678257\n",
      "228.93236249604135\n",
      "228.91152257587186\n",
      "228.89068850569828\n",
      "228.86986028390228\n",
      "228.8490379088656\n",
      "228.82822137897193\n",
      "228.80741069260316\n",
      "228.78660584814315\n",
      "228.7658068439759\n",
      "228.7450136784859\n",
      "228.72422635005773\n",
      "228.7034448570765\n",
      "228.682669197928\n",
      "228.66189937099838\n",
      "228.64113537467423\n",
      "228.62037720734259\n",
      "228.59962486739084\n",
      "228.57887835320648\n",
      "228.55813766317863\n",
      "228.53740279569567\n",
      "228.51667374914697\n",
      "228.49595052192183\n",
      "228.47523311241082\n",
      "228.45452151900372\n",
      "228.4338157400925\n",
      "228.41311577406796\n",
      "228.3924216193221\n",
      "228.37173327424762\n",
      "228.35105073723662\n",
      "228.33037400668286\n",
      "228.30970308097992\n",
      "228.28903795852105\n",
      "228.26837863770217\n",
      "228.24772511691722\n",
      "228.22707739456166\n",
      "228.20643546903204\n",
      "228.1857993387238\n",
      "228.1651690020344\n",
      "228.14454445736052\n",
      "228.12392570309999\n",
      "228.10331273765084\n",
      "228.08270555941093\n",
      "228.06210416678036\n",
      "228.04150855815786\n",
      "228.020918731943\n",
      "228.00033468653643\n",
      "227.97975642033882\n",
      "227.95918393175066\n",
      "227.93861721917432\n",
      "227.91805628101199\n",
      "227.89750111566534\n",
      "227.87695172153735\n",
      "227.85640809703162\n",
      "227.8358702405518\n",
      "227.81533815050236\n",
      "227.79481182528767\n",
      "227.77429126331242\n",
      "227.7537764629828\n",
      "227.73326742270476\n",
      "227.71276414088396\n",
      "227.6922666159278\n",
      "227.671774846244\n",
      "227.651288830239\n",
      "227.6308085663218\n",
      "227.61033405290104\n",
      "227.58986528838508\n",
      "227.56940227118434\n",
      "227.54894499970726\n",
      "227.52849347236636\n",
      "227.50804768757064\n",
      "227.48760764373117\n",
      "227.46717333926054\n",
      "227.44674477257055\n",
      "227.4263219420732\n",
      "227.4059048461825\n",
      "227.38549348331102\n",
      "227.36508785187243\n",
      "227.34468795028133\n",
      "227.32429377695263\n",
      "227.30390533030098\n",
      "227.2835226087421\n",
      "227.2631456106919\n",
      "227.2427743345671\n",
      "227.22240877878437\n",
      "227.202048941761\n",
      "227.18169482191482\n",
      "227.1613464176638\n",
      "227.14100372742664\n",
      "227.12066674962253\n",
      "227.1003354826707\n",
      "227.08000992499095\n",
      "227.05969007500403\n",
      "227.03937593113017\n",
      "227.0190674917918\n",
      "226.99876475540873\n",
      "226.97846772040444\n",
      "226.95817638520091\n",
      "226.93789074822098\n",
      "226.91761080788802\n",
      "226.8973365626261\n",
      "226.87706801085994\n",
      "226.85680515101305\n",
      "226.83654798151133\n",
      "226.8162965007802\n",
      "226.79605070724494\n",
      "226.77581059933297\n",
      "226.75557617547074\n",
      "226.73534743408527\n",
      "226.71512437360468\n",
      "226.69490699245662\n",
      "226.6746952890695\n",
      "226.65448926187372\n",
      "226.63428890929708\n",
      "226.61409422977044\n",
      "226.59390522172322\n",
      "226.57372188358713\n",
      "226.55354421379243\n",
      "226.53337221077135\n",
      "226.51320587295538\n",
      "226.4930451987774\n",
      "226.4728901866704\n",
      "226.45274083506683\n",
      "226.4325971424012\n",
      "226.4124591071077\n",
      "226.39232672762054\n",
      "226.37220000237488\n",
      "226.35207892980634\n",
      "226.33196350835038\n",
      "226.31185373644357\n",
      "226.29174961252292\n",
      "226.27165113502522\n",
      "226.2515583023884\n",
      "226.23147111304968\n",
      "226.21138956544888\n",
      "226.19131365802383\n",
      "226.17124338921425\n",
      "226.15117875745995\n",
      "226.1311197612006\n",
      "226.11106639887743\n",
      "226.09101866893118\n",
      "226.0709765698031\n",
      "226.05094009993596\n",
      "226.0309092577708\n",
      "226.01088404175172\n",
      "225.9908644503205\n",
      "225.97085048192176\n",
      "225.95084213499914\n",
      "225.93083940799704\n",
      "225.9108422993602\n",
      "225.89085080753452\n",
      "225.87086493096552\n",
      "225.8508846680995\n",
      "225.83091001738222\n",
      "225.81094097726154\n",
      "225.79097754618456\n",
      "225.771019722599\n",
      "225.75106750495397\n",
      "225.73112089169703\n",
      "225.71117988127844\n",
      "225.69124447214708\n",
      "225.67131466275302\n",
      "225.65139045154692\n",
      "225.63147183697936\n",
      "225.611558817502\n",
      "225.59165139156605\n",
      "225.57174955762412\n",
      "225.55185331412855\n",
      "225.53196265953198\n",
      "225.51207759228865\n",
      "225.49219811085155\n",
      "225.47232421367562\n",
      "225.4524558992148\n",
      "225.4325931659248\n",
      "225.41273601226092\n",
      "225.3928844366793\n",
      "225.37303843763627\n",
      "225.35319801358818\n",
      "225.33336316299275\n",
      "225.31353388430736\n",
      "225.29371017599019\n",
      "225.2738920364999\n",
      "225.2540794642955\n",
      "225.23427245783552\n",
      "225.21447101558095\n",
      "225.19467513599062\n",
      "225.1748848175264\n",
      "225.1551000586487\n",
      "225.13532085781873\n",
      "225.11554721349924\n",
      "225.09577912415187\n",
      "225.07601658823873\n",
      "225.0562596042245\n",
      "225.0365081705718\n",
      "225.01676228574487\n",
      "224.99702194820765\n",
      "224.97728715642538\n",
      "224.95755790886412\n",
      "224.93783420398796\n",
      "224.91811604026407\n",
      "224.89840341615874\n",
      "224.87869633013833\n",
      "224.85899478067157\n",
      "224.83929876622497\n",
      "224.81960828526726\n",
      "224.79992333626691\n",
      "224.780243917693\n",
      "224.76057002801494\n",
      "224.74090166570338\n",
      "224.72123882922736\n",
      "224.70158151705826\n",
      "224.68192972766767\n",
      "224.6622834595262\n",
      "224.64264271110665\n",
      "224.62300748088072\n",
      "224.6033777673221\n",
      "224.58375356890332\n",
      "224.56413488409842\n",
      "224.54452171138163\n",
      "224.52491404922662\n",
      "224.5053118961092\n",
      "224.4857152505046\n",
      "224.46612411088833\n",
      "224.44653847573687\n",
      "224.42695834352597\n",
      "224.40738371273355\n",
      "224.38781458183715\n",
      "224.3682509493139\n",
      "224.3486928136427\n",
      "224.32914017330177\n",
      "224.30959302677059\n",
      "224.29005137252855\n",
      "224.27051520905533\n",
      "224.2509845348318\n",
      "224.23145934833863\n",
      "224.21193964805656\n",
      "224.19242543246784\n",
      "224.17291670005375\n",
      "224.1534134492978\n",
      "224.13391567868211\n",
      "224.11442338668985\n",
      "224.0949365718053\n",
      "224.07545523251179\n",
      "224.05597936729492\n",
      "224.03650897463893\n",
      "224.01704405302874\n",
      "223.99758460095134\n",
      "223.9781306168921\n",
      "223.95868209933744\n",
      "223.9392390467751\n",
      "223.91980145769233\n",
      "223.9003693305768\n",
      "223.8809426639169\n",
      "223.86152145620107\n",
      "223.84210570591887\n",
      "223.8226954115597\n",
      "223.80329057161308\n",
      "223.78389118457008\n",
      "223.76449724892063\n",
      "223.7451087631567\n",
      "223.72572572576942\n",
      "223.70634813525055\n",
      "223.68697599009377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223.6676092887902\n",
      "223.64824802983446\n",
      "223.62889221171943\n",
      "223.60954183293916\n",
      "223.59019689198882\n",
      "223.57085738736322\n",
      "223.55152331755696\n",
      "223.53219468106647\n",
      "223.5128714763876\n",
      "223.4935537020169\n",
      "223.4742413564517\n",
      "223.45493443818887\n",
      "223.43563294572633\n",
      "223.41633687756303\n",
      "223.39704623219623\n",
      "223.37776100812613\n",
      "223.35848120385182\n",
      "223.33920681787305\n",
      "223.31993784869013\n",
      "223.30067429480374\n",
      "223.28141615471517\n",
      "223.2621634269259\n",
      "223.24291610993765\n",
      "223.2236742022529\n",
      "223.20443770237438\n",
      "223.18520660880523\n",
      "223.16598092004887\n",
      "223.14676063460956\n",
      "223.12754575099183\n",
      "223.1083362676997\n",
      "223.08913218323968\n",
      "223.06993349611596\n",
      "223.05074020483548\n",
      "223.03155230790435\n",
      "223.0123698038297\n",
      "222.99319269111848\n",
      "222.97402096827875\n",
      "222.9548546338181\n",
      "222.93569368624514\n",
      "222.91653812406923\n",
      "222.89738794579958\n",
      "222.87824314994526\n",
      "222.85910373501716\n",
      "222.83996969952562\n",
      "222.82084104198123\n",
      "222.8017177608959\n",
      "222.7825998547809\n",
      "222.76348732214862\n",
      "222.7443801615121\n",
      "222.72527837138364\n",
      "222.70618195027689\n",
      "222.68709089670566\n",
      "222.66800520918434\n",
      "222.6489248862276\n",
      "222.6298499263502\n",
      "222.6107803280676\n",
      "222.59171608989575\n",
      "222.57265721035094\n",
      "222.5536036879503\n",
      "222.53455552121\n",
      "222.51551270864806\n",
      "222.49647524878264\n",
      "222.47744314013127\n",
      "222.45841638121288\n",
      "222.43939497054723\n",
      "222.42037890665318\n",
      "222.40136818805104\n",
      "222.38236281326087\n",
      "222.36336278080353\n",
      "222.34436808920006\n",
      "222.32537873697245\n",
      "222.30639472264215\n",
      "222.28741604473186\n",
      "222.2684427017641\n",
      "222.24947469226223\n",
      "222.23051201474956\n",
      "222.21155466775096\n",
      "222.19260264978965\n",
      "222.17365595939071\n",
      "222.15471459507975\n",
      "222.13577855538236\n",
      "222.11684783882467\n",
      "222.09792244393225\n",
      "222.07900236923282\n",
      "222.06008761325342\n",
      "222.04117817452115\n",
      "222.02227405156526\n",
      "222.0033752429129\n",
      "221.98448174709355\n",
      "221.9655935626362\n",
      "221.946710688071\n",
      "221.9278331219276\n",
      "221.9089608627367\n",
      "221.89009390902868\n",
      "221.87123225933544\n",
      "221.85237591218836\n",
      "221.83352486611926\n",
      "221.81467911966095\n",
      "221.79583867134636\n",
      "221.77700351970898\n",
      "221.75817366328224\n",
      "221.73934910059992\n",
      "221.72052983019682\n",
      "221.70171585060805\n",
      "221.68290716036867\n",
      "221.66410375801402\n",
      "221.64530564208118\n",
      "221.62651281110575\n",
      "221.60772526362507\n",
      "221.58894299817655\n",
      "221.57016601329764\n",
      "221.5513943075266\n",
      "221.53262787940204\n",
      "221.51386672746256\n",
      "221.49511085024767\n",
      "221.4763602462972\n",
      "221.45761491415138\n",
      "221.43887485235035\n",
      "221.42014005943543\n",
      "221.40141053394757\n",
      "221.38268627442906\n",
      "221.3639672794209\n",
      "221.34525354746722\n",
      "221.32654507710967\n",
      "221.3078418668923\n",
      "221.28914391535866\n",
      "221.27045122105238\n",
      "221.25176378251862\n",
      "221.23308159830225\n",
      "221.21440466694833\n",
      "221.19573298700266\n",
      "221.1770665570119\n",
      "221.15840537552165\n",
      "221.13974944107952\n",
      "221.12109875223274\n",
      "221.10245330752915\n",
      "221.0838131055165\n",
      "221.06517814474324\n",
      "221.04654842375896\n",
      "221.02792394111233\n",
      "221.00930469535348\n",
      "220.9906906850321\n",
      "220.97208190869912\n",
      "220.9534783649055\n",
      "220.9348800522025\n",
      "220.9162869691415\n",
      "220.89769911427507\n",
      "220.8791164861554\n",
      "220.86053908333534\n",
      "220.8419669043687\n",
      "220.8233999478088\n",
      "220.80483821220986\n",
      "220.78628169612583\n",
      "220.76773039811266\n",
      "220.74918431672504\n",
      "220.73064345051841\n",
      "220.71210779804923\n",
      "220.69357735787364\n",
      "220.6750521285494\n",
      "220.65653210863294\n",
      "220.63801729668208\n",
      "220.6195076912549\n",
      "220.60100329091017\n",
      "220.58250409420654\n",
      "220.5640100997032\n",
      "220.5455213059596\n",
      "220.5270377115365\n",
      "220.50855931499402\n",
      "220.49008611489262\n",
      "220.4716181097939\n",
      "220.4531552982594\n",
      "220.4346976788509\n",
      "220.41624525013142\n",
      "220.39779801066314\n",
      "220.37935595900976\n",
      "220.36091909373502\n",
      "220.342487413402\n",
      "220.32406091657603\n",
      "220.30563960182187\n",
      "220.28722346770374\n",
      "220.26881251278886\n",
      "220.25040673564123\n",
      "220.23200613482894\n",
      "220.2136107089177\n",
      "220.19522045647503\n",
      "220.17683537606868\n",
      "220.1584554662663\n",
      "220.1400807256364\n",
      "220.1217111527477\n",
      "220.10334674616874\n",
      "220.08498750447004\n",
      "220.06663342622082\n",
      "220.04828450999187\n",
      "220.02994075435353\n",
      "220.01160215787672\n",
      "219.99326871913306\n",
      "219.97494043669508\n",
      "219.9566173091342\n",
      "219.93829933502354\n",
      "219.9199865129357\n",
      "219.9016788414442\n",
      "219.88337631912353\n",
      "219.86507894454752\n",
      "219.8467867162907\n",
      "219.8284996329282\n",
      "219.81021769303516\n",
      "219.7919408951877\n",
      "219.7736692379618\n",
      "219.75540271993432\n",
      "219.73714133968176\n",
      "219.71888509578224\n",
      "219.7006339868126\n",
      "219.68238801135152\n",
      "219.66414716797757\n",
      "219.6459114552693\n",
      "219.62768087180615\n",
      "219.60945541616812\n",
      "219.59123508693526\n",
      "219.57301988268767\n",
      "219.5548098020062\n",
      "219.53660484347316\n",
      "219.518405005669\n",
      "219.50021028717595\n",
      "219.48202068657716\n",
      "219.4638362024544\n",
      "219.44565683339218\n",
      "219.42748257797254\n",
      "219.409313434781\n",
      "219.39114940240069\n",
      "219.37299047941727\n",
      "219.3548366644157\n",
      "219.33668795598103\n",
      "219.3185443526999\n",
      "219.30040585315825\n",
      "219.28227245594286\n",
      "219.2641441596408\n",
      "219.24602096283908\n",
      "219.22790286412715\n",
      "219.20978986209116\n",
      "219.19168195532134\n",
      "219.17357914240594\n",
      "219.155481421935\n",
      "219.1373887924975\n",
      "219.11930125268458\n",
      "219.10121880108608\n",
      "219.0831414362933\n",
      "219.06506915689795\n",
      "219.04700196149128\n",
      "219.02893984866546\n",
      "219.01088281701297\n",
      "218.9928308651276\n",
      "218.97478399160116\n",
      "218.9567421950283\n",
      "218.93870547400314\n",
      "218.92067382711966\n",
      "218.90264725297325\n",
      "218.88462575015902\n",
      "218.866609317272\n",
      "218.84859795290896\n",
      "218.83059165566556\n",
      "218.81259042413967\n",
      "218.79459425692733\n",
      "218.77660315262625\n",
      "218.75861710983503\n",
      "218.74063612715173\n",
      "218.7226602031747\n",
      "218.7046893365029\n",
      "218.68672352573685\n",
      "218.66876276947534\n",
      "218.65080706631875\n",
      "218.63285641486823\n",
      "218.6149108137244\n",
      "218.59697026148913\n",
      "218.57903475676338\n",
      "218.5611042981502\n",
      "218.5431788842513\n",
      "218.5252585136705\n",
      "218.50734318501034\n",
      "218.48943289687497\n",
      "218.47152764786833\n",
      "218.45362743659504\n",
      "218.4357322616595\n",
      "218.41784212166752\n",
      "218.39995701522432\n",
      "218.38207694093595\n",
      "218.36420189740883\n",
      "218.34633188325014\n",
      "218.32846689706682\n",
      "218.31060693746596\n",
      "218.29275200305597\n",
      "218.274902092445\n",
      "218.2570572042416\n",
      "218.23921733705478\n",
      "218.2213824894946\n",
      "218.20355266016998\n",
      "218.18572784769162\n",
      "218.16790805067055\n",
      "218.15009326771693\n",
      "218.13228349744273\n",
      "218.1144787384591\n",
      "218.09667898937872\n",
      "218.07888424881375\n",
      "218.06109451537733\n",
      "218.04330978768274\n",
      "218.02553006434275\n",
      "218.00775534397303\n",
      "217.98998562518645\n",
      "217.97222090659864\n",
      "217.95446118682491\n",
      "217.93670646447998\n",
      "217.9189567381803\n",
      "217.9012120065424\n",
      "217.88347226818246\n",
      "217.8657375217175\n",
      "217.84800776576589\n",
      "217.83028299894457\n",
      "217.81256321987198\n",
      "217.79484842716678\n",
      "217.77713861944795\n",
      "217.75943379533516\n",
      "217.74173395344684\n",
      "217.72403909240495\n",
      "217.70634921082885\n",
      "217.68866430733914\n",
      "217.67098438055817\n",
      "217.65330942910646\n",
      "217.635639451607\n",
      "217.61797444668125\n",
      "217.6003144129531\n",
      "217.58265934904395\n",
      "217.56500925357895\n",
      "217.5473641251818\n",
      "217.5297239624758\n",
      "217.51208876408646\n",
      "217.4944585286383\n",
      "217.4768332547573\n",
      "217.45921294106884\n",
      "217.44159758619895\n",
      "217.42398718877422\n",
      "217.40638174742224\n",
      "217.38878126076978\n",
      "217.37118572744407\n",
      "217.35359514607381\n",
      "217.33600951528769\n",
      "217.3184288337137\n",
      "217.30085309998137\n",
      "217.28328231272016\n",
      "217.26571647056028\n",
      "217.248155572132\n",
      "217.23059961606577\n",
      "217.21304860099298\n",
      "217.1955025255448\n",
      "217.17796138835314\n",
      "217.16042518805023\n",
      "217.1428939232684\n",
      "217.12536759264086\n",
      "217.10784619480117\n",
      "217.09032972838227\n",
      "217.07281819201887\n",
      "217.05531158434513\n",
      "217.0378099039961\n",
      "217.02031314960695\n",
      "217.00282131981297\n",
      "216.98533441325065\n",
      "216.96785242855552\n",
      "216.95037536436493\n",
      "216.93290321931562\n",
      "216.9154359920455\n",
      "216.89797368119193\n",
      "216.88051628539304\n",
      "216.8630638032877\n",
      "216.8456162335146\n",
      "216.82817357471336\n",
      "216.8107358255236\n",
      "216.79330298458535\n",
      "216.77587505053896\n",
      "216.758452022025\n",
      "216.7410338976854\n",
      "216.72362067616095\n",
      "216.70621235609406\n",
      "216.68880893612678\n",
      "216.6714104149017\n",
      "216.65401679106253\n",
      "216.6366280632522\n",
      "216.6192442301145\n",
      "216.60186529029343\n",
      "216.58449124243364\n",
      "216.5671220851808\n",
      "216.54975781717917\n",
      "216.53239843707468\n",
      "216.51504394351383\n",
      "216.49769433514248\n",
      "216.48034961060753\n",
      "216.4630097685567\n",
      "216.44567480763644\n",
      "216.42834472649562\n",
      "216.41101952378216\n",
      "216.3936991981446\n",
      "216.37638374823217\n",
      "216.3590731726936\n",
      "216.34176747017997\n",
      "216.32446663933993\n",
      "216.30717067882512\n",
      "216.28987958728504\n",
      "216.27259336337264\n",
      "216.25531200573872\n",
      "216.23803551303465\n",
      "216.2207638839138\n",
      "216.20349711702835\n",
      "216.18623521103126\n",
      "216.16897816457634\n",
      "216.15172597631755\n",
      "216.13447864490854\n",
      "216.1172361690045\n",
      "216.0999985472597\n",
      "216.08276577832962\n",
      "216.06553786087076\n",
      "216.048314793538\n",
      "216.03109657498854\n",
      "216.0138832038789\n",
      "215.99667467886604\n",
      "215.97947099860758\n",
      "215.96227216176203\n",
      "215.94507816698666\n",
      "215.92788901294102\n",
      "215.91070469828313\n",
      "215.89352522167306\n",
      "215.87635058177065\n",
      "215.85918077723548\n",
      "215.84201580672868\n",
      "215.82485566891032\n",
      "215.80770036244215\n",
      "215.79054988598554\n",
      "215.77340423820272\n",
      "215.75626341775543\n",
      "215.73912742330697\n",
      "215.72199625351968\n",
      "215.704869907058\n",
      "215.6877483825846\n",
      "215.6706316787644\n",
      "215.65351979426205\n",
      "215.63641272774143\n",
      "215.61931047786862\n",
      "215.60221304330918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215.5851204227286\n",
      "215.56803261479425\n",
      "215.550949618172\n",
      "215.5338714315288\n",
      "215.51679805353248\n",
      "215.49972948285125\n",
      "215.48266571815245\n",
      "215.4656067581052\n",
      "215.4485526013786\n",
      "215.4315032466408\n",
      "215.41445869256296\n",
      "215.397418937814\n",
      "215.38038398106465\n",
      "215.3633538209856\n",
      "215.34632845624853\n",
      "215.3293078855243\n",
      "215.31229210748478\n",
      "215.29528112080214\n",
      "215.27827492414923\n",
      "215.26127351619883\n",
      "215.2442768956243\n",
      "215.22728506109934\n",
      "215.21029801129762\n",
      "215.19331574489425\n",
      "215.1763382605629\n",
      "215.1593655569803\n",
      "215.1423976328204\n",
      "215.12543448675962\n",
      "215.10847611747425\n",
      "215.09152252364095\n",
      "215.0745737039365\n",
      "215.05762965703826\n",
      "215.04069038162382\n",
      "215.0237558763714\n",
      "215.00682613995917\n",
      "214.98990117106612\n",
      "214.97298096837125\n",
      "214.95606553055376\n",
      "214.93915485629415\n",
      "214.92224894427252\n",
      "214.90534779316883\n",
      "214.8884514016643\n",
      "214.87155976844042\n",
      "214.85467289217866\n",
      "214.83779077156123\n",
      "214.82091340527046\n",
      "214.8040407919894\n",
      "214.7871729304002\n",
      "214.7703098191878\n",
      "214.7534514570348\n",
      "214.73659784262537\n",
      "214.71974897464537\n",
      "214.7029048517783\n",
      "214.68606547270994\n",
      "214.66923083612605\n",
      "214.6524009407128\n",
      "214.63557578515642\n",
      "214.6187553681432\n",
      "214.60193968836103\n",
      "214.58512874449644\n",
      "214.56832253523788\n",
      "214.55152105927377\n",
      "214.53472431529187\n",
      "214.5179323019814\n",
      "214.5011450180318\n",
      "214.4843624621329\n",
      "214.46758463297388\n",
      "214.45081152924536\n",
      "214.4340431496385\n",
      "214.417279492844\n",
      "214.4005205575535\n",
      "214.38376634245827\n",
      "214.36701684625095\n",
      "214.35027206762325\n",
      "214.33353200526957\n",
      "214.31679665788175\n",
      "214.30006602415364\n",
      "214.2833401027793\n",
      "214.26661889245324\n",
      "214.24990239187002\n",
      "214.2331905997241\n",
      "214.21648351471154\n",
      "214.19978113552807\n",
      "214.1830834608689\n",
      "214.16639048943154\n",
      "214.1497022199122\n",
      "214.13301865100829\n",
      "214.1163397814169\n",
      "214.09966560983636\n",
      "214.08299613496453\n",
      "214.06633135550004\n",
      "214.04967127014265\n",
      "214.03301587759057\n",
      "214.01636517654416\n",
      "213.99971916570308\n",
      "213.9830778437682\n",
      "213.96644120943915\n",
      "213.94980926141835\n",
      "213.93318199840627\n",
      "213.91655941910548\n",
      "213.89994152221743\n",
      "213.88332830644512\n",
      "213.8667197704913\n",
      "213.85011591305923\n",
      "213.83351673285233\n",
      "213.81692222857473\n",
      "Maximum interations reached\n",
      "Optimization result:\n",
      "\t y =8.402849298501387e-05 + 8.402846965870417e-05(x1) + 8.401832359071815e-05(x2) + 8.401826262702247e-05(x3) + 8.401825484693143e-05(x4) + 8.401760301827112e-05(x5) + 8.401759048284467e-05(x6) + 8.401758085442893e-05(x7) + 8.401706017571798e-05(x8) + 8.401705474821596e-05(x9) + 8.401705471460922e-05(x10) + 8.401705076580766e-05(x11) + 8.40170480436552e-05(x12)\n"
     ]
    }
   ],
   "source": [
    "def f_mod(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, \n",
    "         a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12):\n",
    "    return a0 + a1*x1 + a2*x2 + a3*x3 + a4*x4 + a5*x5 + a6*x6 + a7*x7 + a8*x8 + a9*x9 + a10*x10 + a11*x11 + a12*x12\n",
    "\n",
    "def Loss(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12,\n",
    "         a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, y):\n",
    "    return np.sum(np.square(f_mod(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12,\n",
    "         a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12) - y), axis = 0)\n",
    "\n",
    "d_Loss_a0 = grad(Loss, argnum=14)\n",
    "d_Loss_a1 = grad(Loss, argnum=15)\n",
    "d_Loss_a2 = grad(Loss, argnum=16)\n",
    "d_Loss_a3 = grad(Loss, argnum=17)\n",
    "d_Loss_a4 = grad(Loss, argnum=18)\n",
    "d_Loss_a5 = grad(Loss, argnum=19)\n",
    "d_Loss_a6 = grad(Loss, argnum=20)\n",
    "d_Loss_a7 = grad(Loss, argnum=21)\n",
    "d_Loss_a8 = grad(Loss, argnum=22)\n",
    "d_Loss_a9 = grad(Loss, argnum=23)\n",
    "d_Loss_a9 = grad(Loss, argnum=24)\n",
    "d_Loss_a10 = grad(Loss, argnum=25)\n",
    "d_Loss_a11 = grad(Loss, argnum=26)\n",
    "d_Loss_a12 = grad(Loss, argnum=27)\n",
    "\n",
    "\n",
    "\n",
    "par_a_0 = 0.\n",
    "par_a_1 = 0.\n",
    "par_a_2 = 0.\n",
    "par_a_3 = 0.\n",
    "par_a_4 = 0.\n",
    "par_a_5 = 0.\n",
    "par_a_6 = 0.\n",
    "par_a_7 = 0.\n",
    "par_a_8 = 0.\n",
    "par_a_9 = 0.\n",
    "par_a_10 = 0.\n",
    "par_a_11 = 0.\n",
    "par_a_12 = 0.\n",
    "\n",
    "\n",
    "learning_rate = 1e-10\n",
    "tolerance = 10\n",
    "max_iter = 1e3\n",
    "cont = 0\n",
    "\n",
    "while (tolerance < Loss(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train )):\n",
    "    par_a_0 = par_a_0 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_1 = par_a_1 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_2 = par_a_2 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_3 = par_a_3 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_4 = par_a_4 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_5 = par_a_5 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_6 = par_a_6 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_7 = par_a_7 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_8 = par_a_8 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_9 = par_a_9 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_10 = par_a_10 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_11 = par_a_11 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_12 = par_a_12 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    par_a_13 = par_a_13 - learning_rate * d_Loss_a0(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train)\n",
    "    cont += 1\n",
    "    print(Loss(x_train[:,0], x_train[:,1], x_train[:,2], x_train[:,3], x_train[:,4], x_train[:,5], x_train[:,6], x_train[:,7], x_train[:,8], x_train[:,9], x_train[:,10], x_train[:,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12, y_train ))\n",
    "    if cont > max_iter:\n",
    "        print('Maximum interations reached')\n",
    "        break\n",
    "print('Optimization result:\\n\\t y ='+ str(par_a_0) + ' + '+str(par_a_1)+'(x1) + '+str(par_a_2)+'(x2) + '+str(par_a_3)+'(x3) + '+str(par_a_4)+'(x4) + '+str(par_a_5)+'(x5) + '+str(par_a_6)+'(x6) + '+str(par_a_7)+'(x7) + '+str(par_a_8)+'(x8) + '+str(par_a_9)+'(x9) + '+str(par_a_10)+'(x10) + '+str(par_a_11)+'(x11) + '+str(par_a_12)+'(x12)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to try our model, we get the predictions from our X_test data to compare it with y_test\n",
    "reg_mod_pred = []\n",
    "for i in range(len(x_test)):\n",
    "    if f_mod(x_test[i,0], x_test[i,1], x_test[i,2], x_test[i,3], x_test[i,4], x_test[i,5], x_test[i,6], x_test[i,7], x_test[i,8], x_test[i,9], x_test[i,10], x_test[i,11], par_a_0, par_a_1, par_a_2, par_a_3, par_a_4, par_a_5, par_a_6, par_a_7, par_a_8, par_a_9, par_a_10, par_a_11, par_a_12) < 0.5:\n",
    "        reg_mod_pred.append(0)\n",
    "    else:\n",
    "        reg_mod_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validations MAE:  109 / 291 = 0.3745704467353952\n"
     ]
    }
   ],
   "source": [
    "val_mae_regmodel = mean_absolute_error(y_test, reg_mod_pred)\n",
    "counter_difpreds = 0\n",
    "for e in range(len(reg_mod_pred)):\n",
    "    if reg_mod_pred[e] != y_test[e]:\n",
    "        counter_difpreds += 1\n",
    "print('Validations MAE:  {} / 291 = {}'.format(counter_difpreds, val_mae_regmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine Learning scoring parameters\n",
    "\n",
    "There are different scoring metrics to validate the quality of ML models. In our models we have measured the accuracy as number of correctly predicted outputs vs. total outputs. However, for statistical analysis of binary classification, there's an interesting method called 'F1 score' or 'F score'.\n",
    "\n",
    "https://en.wikipedia.org/wiki/F1_score\n",
    "\n",
    "As previously stated, the intent of this kernel is not to get into much detail about ML (methods, quality scores, etc.) so we are just pointing the scoring methods out for future consideration.\n",
    "\n",
    "In terms of accuracy, with the feature engineering we have applied to our data the gression model is the one that yields better results so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare results for submission\n",
    "\n",
    "In terms of accuracy, the model that yields the best results (for the dataset of the project and the data cleaning we have applied) is the XGBoost model.\n",
    "\n",
    "Let's use it to submit our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mod_traind_ohe[cols_sorted].values\n",
    "separate = lambda x: (x[:,1:], x[:,0])\n",
    "x_train, y_train = separate(data)\n",
    "\n",
    "X_test_data = mod_testd_ohe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(x_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictions = xgb_model.predict(X_test_data)\n",
    "Y_test = []\n",
    "for i in range(len(X_test_data)):\n",
    "    if xgb_predictions[i] < 0.5:\n",
    "        Y_test.append(0)\n",
    "    else:\n",
    "        Y_test.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "208         1100         1\n",
       "407         1299         0\n",
       "117         1009         1\n",
       "338         1230         0\n",
       "165         1057         0\n",
       "23           915         1\n",
       "181         1073         0\n",
       "74           966         1\n",
       "84           976         0\n",
       "389         1281         0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We prepare a new dataframe with the results and check it to make sure it has the correct format:\n",
    "submission_titanic = pd.DataFrame(np.hstack([np.array(mod_testd_ohe['PassengerId']).reshape(-1, 1), pd.DataFrame({'Survived': Y_test})]))\n",
    "submission_titanic.columns = ['PassengerId', 'Survived']\n",
    "submission_titanic.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_titanic.to_csv('titanic_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
